{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSz+/bvaiWd+jGd6qK/KRR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JungCesar/bscaithesis/blob/master/final_bsc_ai_thesis_mfcc_1dconv_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dysarthria Classification Algorithm\n",
        "## Features: MFCCs\n",
        "## Classifier: 1D CONV\n",
        "\n",
        "In this notebook, I will create a dyasarthria classification algorithm for my bachelor's thesis Artificial Intelligence at the University of Amsterdam. Dysarthria occurs when the muscles you use for speech are weak or you have difficulty controlling them. The title of my thesis is \"Investigating Pre-Trained Self-Supervised Deep Learning Models for Disease Recognition\". The idea is to apply the algorithm created here to other datasets as well, for example audiofiles from the DementiaBank databases.\n",
        "\n",
        "The [Torgo](http://www.cs.toronto.edu/~complingweb/data/TORGO/torgo.html) dataste perfectly fits this purpose. It consists of dyasrthric male and female speakers, either caused by cerebral palsy (CP) or amyotrophic lateral sclerosis (ALS) and their non-dysarthric counterparts, also known as the healthy control group.\n",
        "\n",
        "I will exrtact 30 MFCCs using Python's Librosa library. Then I will train a 1d convolutional network on the binary classification between 'dysarthria' and 'non-dysarthria'.\n",
        "\n",
        "I will take a bottom-up approach, try to be as operating system-independent as possible, and explain the steps in detail below. The structure will be as follows:\n",
        "\n",
        "1.   Installing and Exlpaining Necessary Packages\n",
        "2.   Downloading, Storing, and Accessing (Loading) the Dataset\n",
        "3.   Exploratory Data Analysis (EDA)\n",
        "4.   Dataset Preprocessing\n",
        "5.   Dataset Splitting (into a train- and test set)\n",
        "6.   MFCC Feature Extraction\n",
        "7.   Training 1d Convolutional Classifier\n",
        "8.   Model Evaluation\n",
        "\n",
        "Notes:\n",
        "\n",
        "*   The above link to TORGO redirects to the Computational Linguistics website of the university of Toronto. An in-depth explanation can be found there, but the actual dataset that will be used in this notebook, comes from Kaggle. [Kaggle's version of Torgo](https://www.kaggle.com/datasets/iamhungundji/dysarthria-detection) is already some sort of preprocessed form of the original one.\n"
      ],
      "metadata": {
        "id": "8sq8VcumHuyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, I install the necessary libraries and use `%%capture` to hide all the output."
      ],
      "metadata": {
        "id": "uFqGjrR_KgWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip3 install librosa numpy scikit-learn"
      ],
      "metadata": {
        "id": "hYWoKnJ3AkL1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, I mount, or say connect to, my Google Drive, where my data is being stored. This is the easiest way to access your data in Google Colab without the need of uploading everything from tuntime to runtime, but when working in a local Jupyter Notebook and having your data stored locally on your machine, one could replace these lines with simply accessing your local files."
      ],
      "metadata": {
        "id": "UqgTaeKiK2oT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VSvbqnpceBhm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca85add-d3f1-48c4-e9c0-f79822b0fdae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Get access to personal Google Drive account\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the CNN requires the input to be of a constant shape, I will a second from the exact middle of all audio files that have a duration of longer than 1 second. All files with a duration of less than one second will be discarded."
      ],
      "metadata": {
        "id": "7btm77Df9gq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# str(path) returns something like: /content/drive/MyDrive/bsc-ai-thesis/torgo_data/dysarthria_female/F01_Session1_0002.wav\n",
        "# tqdm is used to create a smart progress bar for the loops, for example it shows loading time\n",
        "import librosa\n",
        "import torchaudio\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "data = []\n",
        "\n",
        "# Loop through all audio files within the TORGO database\n",
        "for path in tqdm(Path(\"/content/drive/MyDrive/bsc-ai-thesis/torgo_data\").glob(\"**/*.wav\")):\n",
        "\n",
        "    name = str(path).split('/')[-1].split('.')[0]\n",
        "    label = str(path).split('/')[-2]\n",
        "\n",
        "    try:\n",
        "        # Set standard offset for files with duration of 1s\n",
        "        offset = 0\n",
        "\n",
        "        # Option to resample the audio  to 16kHz, same as wav2vec expects\n",
        "        # Load the entire audio file\n",
        "        y_temp, sr_temp = librosa.load(path=path, sr=16000)\n",
        "\n",
        "        # Option to resample the audio  to 16kHz, same as wav2vec expects\n",
        "\n",
        "        # Get the total duration\n",
        "        total_duration = librosa.get_duration(y=y_temp, sr=sr_temp)\n",
        "\n",
        "        if total_duration < 1:\n",
        "            continue\n",
        "\n",
        "        if total_duration > 1:\n",
        "            # Calculate the offset to start from the middle of the file\n",
        "            offset = (total_duration - 1) / 2\n",
        "\n",
        "        # Load 1 second from the middle of the audio file\n",
        "        y, sr = librosa.load(path=path, sr=16000, offset=offset, duration=1)\n",
        "\n",
        "        # Try loading all files here, when broken this will jump to exception\n",
        "        s = torchaudio.load(path)\n",
        "        data.append({\n",
        "            \"filename\": name,\n",
        "            \"path\": str(path),\n",
        "            \"y\": y,\n",
        "            \"sr\": sr,\n",
        "            \"dysarthria_class\": label\n",
        "        })\n",
        "\n",
        "    # Excpetions will be elaborated here\n",
        "    except Exception as e:\n",
        "        print(\"\\n\", e, str(path))\n",
        "        pass"
      ],
      "metadata": {
        "id": "lq4GcBc9GILo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "710864cd-e243-4fcd-e9c7-086db09a46ae"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "79it [00:00, 129.00it/s]<ipython-input-33-406dd4128d0f>:22: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y_temp, sr_temp = librosa.load(path=path, sr=16000)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "114it [00:01, 150.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  /content/drive/MyDrive/bsc-ai-thesis/torgo_data/dysarthria_female/F01_Session1_0068.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2000it [00:14, 139.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I convert the data to a pandas dataframe and show the first five instances to visually see the data that I am dealing with."
      ],
      "metadata": {
        "id": "baKi-fI3MxE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show how the Pandas dataframe looks like currently\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "o7hQerhGbT5H",
        "outputId": "b81d7b36-af80-474d-bdb2-91c495345f78"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            filename                                               path  \\\n",
              "0  F01_Session1_0006  /content/drive/MyDrive/bsc-ai-thesis/torgo_dat...   \n",
              "1  F01_Session1_0038  /content/drive/MyDrive/bsc-ai-thesis/torgo_dat...   \n",
              "2  F01_Session1_0015  /content/drive/MyDrive/bsc-ai-thesis/torgo_dat...   \n",
              "3  F01_Session1_0024  /content/drive/MyDrive/bsc-ai-thesis/torgo_dat...   \n",
              "4  F01_Session1_0053  /content/drive/MyDrive/bsc-ai-thesis/torgo_dat...   \n",
              "\n",
              "                                                   y     sr   dysarthria_class  \n",
              "0  [0.0011291504, 0.00036621094, 0.0019836426, 0....  16000  dysarthria_female  \n",
              "1  [0.13388062, 0.09274292, 0.10745239, 0.1221313...  16000  dysarthria_female  \n",
              "2  [-0.0020446777, -0.0021362305, -0.0021972656, ...  16000  dysarthria_female  \n",
              "3  [-0.0012512207, -0.0011291504, -0.0011901855, ...  16000  dysarthria_female  \n",
              "4  [-0.00064086914, -0.00048828125, -0.0004577636...  16000  dysarthria_female  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d8795fc-349d-4ffc-aaa9-1b69d42b4bb5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>path</th>\n",
              "      <th>y</th>\n",
              "      <th>sr</th>\n",
              "      <th>dysarthria_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>F01_Session1_0006</td>\n",
              "      <td>/content/drive/MyDrive/bsc-ai-thesis/torgo_dat...</td>\n",
              "      <td>[0.0011291504, 0.00036621094, 0.0019836426, 0....</td>\n",
              "      <td>16000</td>\n",
              "      <td>dysarthria_female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>F01_Session1_0038</td>\n",
              "      <td>/content/drive/MyDrive/bsc-ai-thesis/torgo_dat...</td>\n",
              "      <td>[0.13388062, 0.09274292, 0.10745239, 0.1221313...</td>\n",
              "      <td>16000</td>\n",
              "      <td>dysarthria_female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>F01_Session1_0015</td>\n",
              "      <td>/content/drive/MyDrive/bsc-ai-thesis/torgo_dat...</td>\n",
              "      <td>[-0.0020446777, -0.0021362305, -0.0021972656, ...</td>\n",
              "      <td>16000</td>\n",
              "      <td>dysarthria_female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>F01_Session1_0024</td>\n",
              "      <td>/content/drive/MyDrive/bsc-ai-thesis/torgo_dat...</td>\n",
              "      <td>[-0.0012512207, -0.0011291504, -0.0011901855, ...</td>\n",
              "      <td>16000</td>\n",
              "      <td>dysarthria_female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>F01_Session1_0053</td>\n",
              "      <td>/content/drive/MyDrive/bsc-ai-thesis/torgo_dat...</td>\n",
              "      <td>[-0.00064086914, -0.00048828125, -0.0004577636...</td>\n",
              "      <td>16000</td>\n",
              "      <td>dysarthria_female</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d8795fc-349d-4ffc-aaa9-1b69d42b4bb5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d8795fc-349d-4ffc-aaa9-1b69d42b4bb5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d8795fc-349d-4ffc-aaa9-1b69d42b4bb5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, my goal is to train a model to recognize the presence or absence of dysarthria in speech, it would be appropriate to combine the four data folders into two categories: patients *with* the disease and patients *without* the disease. This will simplify the training process and ensure that the model is focused on recognizing the presence of dysarthria, instead of recognizing a gender. Let's see how many audio files each of the now two categories contain.\n",
        "\n",
        "It is noticeable that there was one instance of audio filtered out previously, specifically an instance of 'dysarthria'. The 1000 samples for each class, were reduced to 999 for 'dysarthria'."
      ],
      "metadata": {
        "id": "SGwD7TWdMAdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminate difference between male and female and print distribbution\n",
        "df = df.replace({'dysarthria_class' : {'dysarthria_female': 'dysarthria', 'dysarthria_male': 'dysarthria', 'non_dysarthria_female': 'non_dysarthria', 'non_dysarthria_male': 'non_dysarthria'}})\n",
        "print(\"Labels: \", df[\"dysarthria_class\"].unique())\n",
        "print()\n",
        "df.groupby('dysarthria_class').count()"
      ],
      "metadata": {
        "id": "Z0JR61h0u48C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "e34d63ca-77bb-48a2-9896-1d8348cc8225"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels:  ['dysarthria' 'non_dysarthria']\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  filename  path    y   sr\n",
              "dysarthria_class                          \n",
              "dysarthria             970   970  970  970\n",
              "non_dysarthria         997   997  997  997"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0cf864e8-f670-43ec-b15a-f5ab501f91a0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>path</th>\n",
              "      <th>y</th>\n",
              "      <th>sr</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dysarthria_class</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>dysarthria</th>\n",
              "      <td>970</td>\n",
              "      <td>970</td>\n",
              "      <td>970</td>\n",
              "      <td>970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>non_dysarthria</th>\n",
              "      <td>997</td>\n",
              "      <td>997</td>\n",
              "      <td>997</td>\n",
              "      <td>997</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cf864e8-f670-43ec-b15a-f5ab501f91a0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0cf864e8-f670-43ec-b15a-f5ab501f91a0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0cf864e8-f670-43ec-b15a-f5ab501f91a0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, I want to see how the data in the 'path' column looks like."
      ],
      "metadata": {
        "id": "USKCgPhCNpnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['path'][0])"
      ],
      "metadata": {
        "id": "8MPz3auw0Lip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2733765-7793-4260-e1fa-0485b31a95b5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/bsc-ai-thesis/torgo_data/dysarthria_female/F01_Session1_0006.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For my classification problem, padding, i.e. adding a specific number or set of numbers to the front or back of your audio sample to reach a specific length, is considered unnatural and therefore not desired. That is why I will remove all audio files that durate less than a second (1s or 1000ms)."
      ],
      "metadata": {
        "id": "PYNWU478N2lW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "\n",
        "# Count how many samples there are of less than 1 second long\n",
        "# In the next section these will be removed\n",
        "lstsmall = []\n",
        "lstlarge = []\n",
        "\n",
        "for i in range(len(df['path'])):\n",
        "    if librosa.get_duration(path=df['path'][i]) < 1:\n",
        "        lstsmall.append(i)\n",
        "    else:\n",
        "        lstlarge.append(i)\n",
        "\n",
        "print(len(lstsmall), lstsmall)\n",
        "print(len(lstlarge), lstlarge)\n",
        "\n",
        "# Removing all audio files with a  duration of less than a second (1s or 1000ms)\n",
        "print(f\"Step 0: {len(df)}\")\n",
        "\n",
        "# Audio files with length of less than 1 second cause error for MFCC extraction\n",
        "# Therefore delete these files\n",
        "df[\"status\"] = df[\"path\"].apply(lambda x: True if (librosa.get_duration(path=x) > 1) else None)\n",
        "df = df.dropna(subset=[\"status\"])\n",
        "df = df.drop(columns=\"status\")\n",
        "\n",
        "print(f\"Step 1: {len(df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9vgs2FM1KK2",
        "outputId": "0300a07e-08de-421e-8194-d40f7ef564ae"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 []\n",
            "1967 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966]\n",
            "Step 0: 1967\n",
            "Step 1: 1967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(df[\"y\"][0].shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL3t8qj9_Ot4",
        "outputId": "ed13113e-2d4f-4649-ab71-e56660ae4289"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X))\n",
        "print(X[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELxXQawVKvzD",
        "outputId": "165864a0-2ade-469d-b221-7e35fbacd130"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1967\n",
            "(30, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for f in df[\"y\"]:\n",
        "    if str(f.shape) != \"(16000,)\":\n",
        "        print(\"err\")"
      ],
      "metadata": {
        "id": "34qx-v3rP1e3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to extract the MFCCs of a specific files, specified by its path\n",
        "def extract_mfcc(filename):\n",
        "    y, sr  = librosa.load(filename)\n",
        "\n",
        "    # mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=30).T, axis=0)\n",
        "    # return mfcc\n",
        "\n",
        "    # What is the difference of taking the mean of the transpose here instead of just keeping MFCC?????\n",
        "    # Each coefficient will be averaged over time.\n",
        "    # Make sure successive windows overlap slightly\n",
        "    # mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, n_fft=512, center=True).T, axis=0)\n",
        "\n",
        "    # mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr).T, axis=0)\n",
        "    # return mfcc\n",
        "\n",
        "    return librosa.feature.mfcc(y=y, sr=sr, n_mfcc=30, n_fft=512)\n",
        "    # return mfcc"
      ],
      "metadata": {
        "id": "rjOYCWm_b-ck"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_size=0.2\n",
        "\n",
        "X, y = [], []\n",
        "# df = df.reset_index()  # make sure indexes pair with number of rows\n",
        "for _, row in df.iterrows():\n",
        "    # Get the MFCC for all audio files\n",
        "    features = librosa.feature.mfcc(y=row['y'], sr=row['sr'], n_mfcc=30, n_fft=512)\n",
        "\n",
        "    # Get the corresponding label: 'dysarthria' or 'non_dysarthria'\n",
        "    label = row['dysarthria_class']\n",
        "\n",
        "    # add to data\n",
        "    X.append(features)\n",
        "    y.append(label)"
      ],
      "metadata": {
        "id": "Wa6Whwv-zK28"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X)\n",
        "# for x in X:\n",
        "#     print(x.shape)\n",
        "\n",
        "print(type(X))\n",
        "\n",
        "# print(X[0])\n",
        "# print(type(X[0]))\n",
        "# print(len(X[0]))\n",
        "# print(X[0][0])\n",
        "# print(type(X[0][0]))\n",
        "# print(len(X[0][0]))\n",
        "\n",
        "print(X[0].shape)\n",
        "\n",
        "# for x in X:\n",
        "#     if str(X.shape) != \"(30, 44)\":\n",
        "#         print(\"err\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3YEwBEORiEo",
        "outputId": "c7219fcd-46b8-49be-9939-31f1480ff694"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "(30, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(y))\n",
        "print(type(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1h2re4HHVbv",
        "outputId": "80ac0daa-f879-4212-b01d-845ceb817860"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1967\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Transform labels to integers\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)  # this will transform 'non_dysarthria' to 0 and 'dysarthria' to 1"
      ],
      "metadata": {
        "id": "uiHxFlucRgOa"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Assuming you have your features in X and labels in y\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the splits to PyTorch tensors\n",
        "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
        "\n",
        "batch_size = 32\n",
        "# Create data loaders\n",
        "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "DgIB2dPmQ5yN"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Convolutional layer with 30 input channels, 32 output channels, and a kernel size of 3\n",
        "        self.conv1 = nn.Conv1d(in_channels=30, out_channels=32, kernel_size=3)\n",
        "\n",
        "        # Maxpool layer with size 2\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "\n",
        "        # Convolutional layer with 32 input channels, 64 output channels, and a kernel size of 3\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "\n",
        "        # Fully connected layer with input size that will be determined later, and output size of 2 (since we have two classes to predict)\n",
        "        self.fc1 = nn.Linear(in_features=384, out_features=2)  # in_features needs to be updated depending on your data and layers configuration.\n",
        "\n",
        "        # Dropout layer with p=0.5 (probability of an element to be zeroed)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "        # Flatten the output for each sample\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # print(\"Shape before FC layer: \", x.shape)  # add this line to check the shape\n",
        "\n",
        "        # Pass the output through the fully connected layer\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Step 3: Define a loss function and optimizer\n",
        "model = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# lr=0.001, lr=3e-05\n",
        "# optimizer = optim.SGD(model.parameters(), lr=3e-05, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
        "\n",
        "# Step 4: Train the network\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 20 == 19:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA_fOzvtQKya",
        "outputId": "2656a9a2-79b3-4b9e-e456-7c56326bba75"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    # .to(deivce)\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs = inputs\n",
        "            labels = labels\n",
        "\n",
        "            outputs = model(inputs)  # Forward pass\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Get predictions\n",
        "\n",
        "            y_true.extend(labels.tolist())\n",
        "            y_pred.extend(predicted.tolist())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='binary')\n",
        "    recall = recall_score(y_true, y_pred, average='binary')\n",
        "    f1 = f1_score(y_true, y_pred, average='binary')\n",
        "    auc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "    print(f'Accuracy: {100*accuracy:.2f}\\nPrecision: {100*precision:.2f}\\nRecall: {100*recall:.2f}\\nF1-score: {100*f1:.2f}\\nAUC-ROC: {100*auc:.2f}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "evaluate_model(model, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XZQvWxCdj1F",
        "outputId": "093969eb-32c1-41d5-ddaf-b4760c998f4a"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 90.10\n",
            "Precision: 85.09\n",
            "Recall: 97.49\n",
            "F1-score: 90.87\n",
            "AUC-ROC: 90.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        output = model(inputs)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {100 * correct / total}')"
      ],
      "metadata": {
        "id": "3xXlKW-g3tJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b559eb-0226-4398-a531-cbd68c7f6a7d"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 89.08629441624366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import accuracy_score # to measure how good we are\n",
        "\n",
        "# # harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0\n",
        "# from sklearn.metrics import f1_score # F1 = 2 * (precision * recall) / (precision + recall)\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# # predict 25% of data to measure how good we are\n",
        "# y_pred = model.predict(X_test)\n",
        "\n",
        "# # calculate the accuracy\n",
        "# accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
        "# print(\"Accuracy: {:.2f}%\".format(accuracy*100))\n",
        "\n",
        "# f1 = f1_score(y_true=y_test, y_pred=y_pred, labels=['dysarthria', 'non_dysarthria'], average='binary', pos_label='dysarthria')\n",
        "# print(f\"f1: {f1}\")\n",
        "\n",
        "# y_proba = model.predict_proba(X_test)\n",
        "\n",
        "# roc_auc = roc_auc_score(y_true=y_test,  y_score=y_proba[:, 1])\n",
        "# print(f\"ROC AUC: {roc_auc}\")"
      ],
      "metadata": {
        "id": "RvDcCd2Q33m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_data(test_size=0.2):\n",
        "#     X, y = [], []\n",
        "#     df = data.reset_index()  # make sure indexes pair with number of rows\n",
        "#     for _, row in df.iterrows():\n",
        "#         # print(row['path'], row['dysarthria_class'])\n",
        "#         # get the base name of the audio file\n",
        "#         # basename = os.path.basename(file)\n",
        "#         # extract speech features\n",
        "#         features = extract_mfcc(row['path'])\n",
        "#         # get the emotion label\n",
        "#         label = row['dysarthria_class']\n",
        "#         # add to data\n",
        "#         X.append(features)\n",
        "#         y.append(label)\n",
        "#     # split the data to training and testing and return it\n",
        "#     return train_test_split(np.array(X), y, test_size=test_size)"
      ],
      "metadata": {
        "id": "-rnro7-a36v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iZS-ykNN8Vdy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}